{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and cross entropy of \"High Flight\"\n",
    "\n",
    "High Flight is a famous poem. Read in the contents of 'HighFlight.txt' as a list of characters. Calculate the following.\n",
    "\n",
    "1) What is the entropy of the characters in this poem (including the special characters for simplicity)? \n",
    "\n",
    "2) Suppose these characters appeared uniformly, you would expect the entropy to be higher. What would the entropy for the set of characters contained in this poem be if they were uniformly distributed.\n",
    "\n",
    "3) Let p be the frequency distribution of characters in the poem and q be the uniform frequency distribution of those characters. Calculate H(p,q) and H(q,p) (the respective cross entropies) along with KL(p||q) and KL(q||p) (the KL divergences). Verify that H(p,q) = H(p) + KL(p||q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Dropbox/All_Work_Files/Teaching/2024/Q520/HW/Q520_Winter24_Env`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"../../Q520_Winter24_Env\")\n",
    "using StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet of symbols in the string:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set(['E', 'i', 's', ',', 'O', '\\'', 'W', 'T', 'y', 'k', 'S', 'w', 'r', ';', 'c', 'U', 'e', 'j', 'n', 'f', 'M', '-', 'b', '!', 'A', ' ', 'g', 'h', 'P', 'I', 'H', 'a', 'p', 'd', 'G', 't', '.', 'v', 'm', 'o', '\\r', 'u', 'Y', 'l'])\n"
     ]
    }
   ],
   "source": [
    "# Here is some seed code to open the file, get the text, and collect the set of unique symbols\n",
    "\n",
    "f = open(\"HighFlight.txt\", \"r\")\n",
    "contents = read(f, String)\n",
    "close(f)\n",
    "\n",
    "stList = collect(contents)\n",
    "alphabet = Set(stList)  # set of symbols in the string\n",
    "println(\"Alphabet of symbols in the string:\")\n",
    "println(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "poem_length = length(contents)\n",
    "num_chars = length(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Entropy for characters in poem:\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `p` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `p` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Dropbox/All_Work_Files/Teaching/2024/Q520/HW/InformationTheory/Problem_Notebooks/Text_Entropy.ipynb:4"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "println(\"1. Entropy for characters in poem:\")\n",
    "entropy = - sum(p .* log.(p))\n",
    "println(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Entropy if each character was equally likely:\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `entropy_uni` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `entropy_uni` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Dropbox/All_Work_Files/Teaching/2024/Q520/HW/InformationTheory/Problem_Notebooks/Text_Entropy.ipynb:3"
     ]
    }
   ],
   "source": [
    "\n",
    "println(\"2. Entropy if each character was equally likely:\")\n",
    "println(entropy_uni)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropies:\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `H_pq` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `H_pq` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Dropbox/All_Work_Files/Teaching/2024/Q520/HW/InformationTheory/Problem_Notebooks/Text_Entropy.ipynb:3"
     ]
    }
   ],
   "source": [
    "# Cross entropies:\n",
    "println(\"Cross entropies:\")\n",
    "println(\"H(p, q): \", H_pq)\n",
    "println(\"H(q, p): \", H_qp)\n",
    "println()\n",
    "\n",
    "# KLs:\n",
    "println(\"KL divergences:\")\n",
    "println(\"KL(p || q): \", KL_pq)\n",
    "println(\"KL(q || p): \", KL_qp)\n",
    "println()\n",
    "\n",
    "# The rest\n",
    "println(\"From direct calculation we find:\")\n",
    "println(\"H(p, q) = \", H_pq)\n",
    "println(\"Composing entropy and KL yields:\")\n",
    "println(\"H(p) + KL(p||q) = \", ...)\n",
    "println(\"Very small difference: \", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
